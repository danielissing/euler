# LLM Performance on Project Euler Problems

This repository contains code generated by various state-of-the-art LLMs (ChatGPT 5 Pro, Gemini 2.5 Pro, and Claude Opus 4.1) when attempting to solve Project Euler problems across different difficulty levels. Findings hold as of September 2025.

## Project Overview

An experimental evaluation of how well modern AI models handle mathematical and algorithmic challenges. Each model was given the same standardized prompt and asked to solve problems ranging from 5% to 100% difficulty on the Project Euler scale. The goal was to assess not just whether they could solve the problems, but how they approached them—their mathematical insights, coding efficiency, and problem-solving strategies.

## Disclaimer

I'm aware that Project Euler discourages posting solutions online to preserve the learning experience for others. To respect this:

- Problem names or IDs are not used in file titles or folder names
- Problems are referenced only by difficulty level
- I do not specify which solutions were actually correct

While it wouldn't be impossible to figure out which problem is which, it requires deliberate effort. **If you're looking to solve Project Euler problems yourself, I strongly encourage you not to look at this repository.** The purpose here is to document AI capabilities on mathematical problem-solving, not to help anyone shortcut their own learning journey.

## Purpose

This is a research project examining the current state of LLM reasoning capabilities on well-defined mathematical/algorithmic problems, not a solutions database. The interesting findings are about how different models approach problems, their failure modes, and what this tells us about machine intelligence—not the solutions themselves.